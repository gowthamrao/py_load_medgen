{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"py-load-medgen","text":"<p>A Python package for scalable, reliable, and high-performance ETL of the NCBI MedGen database.</p> <p><code>py-load-medgen</code> is a command-line tool designed to download, parse, and load the entire NCBI MedGen dataset into a relational database, with initial support optimized for PostgreSQL. It is built to be extensible, allowing for future integrations with other database systems.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>High-Performance Loading: Utilizes native database bulk loading protocols (e.g., PostgreSQL <code>COPY</code>) for maximum efficiency.</li> <li>Robust Downloading: Includes automatic retries with exponential backoff and download resumption for handling unstable network connections.</li> <li>Data Integrity: Verifies file integrity using MD5 checksums and ensures transactional, all-or-nothing data loads.</li> <li>Full and Delta Modes: Supports both complete refreshes of the dataset and incremental updates.</li> <li>Extensible Architecture: A clean, modular design allows for new database loaders to be added with minimal effort.</li> <li>Comprehensive Logging: Captures detailed metadata about each ETL run for auditability and monitoring.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started, see the CLI Reference for installation and usage instructions.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>The <code>py-load-medgen</code> package is designed with a modular, decoupled architecture to promote maintainability, testability, and extensibility. The core logic is separated into four main components, managed by a central orchestrator.</p>"},{"location":"architecture/#core-components","title":"Core Components","text":"<ol> <li> <p>Orchestrator (<code>cli.py</code>)</p> <ul> <li>Responsibility: Manages the end-to-end ETL workflow.</li> <li>Details: The Command-Line Interface (CLI) acts as the entry point. It parses user arguments, coordinates the other components, and ensures the ETL process runs in the correct sequence (Download -&gt; Parse -&gt; Load). It is also responsible for high-level error handling and logging the final status of the run.</li> </ul> </li> <li> <p>Downloader (<code>downloader.py</code>)</p> <ul> <li>Responsibility: Acquiring and verifying data from the NCBI FTP server.</li> <li>Details: This module handles all FTP interactions. It is built to be robust, with features like connection retries, download resumption, and MD5 checksum validation to ensure the integrity of the source data.</li> </ul> </li> <li> <p>Parser / Transformer (<code>parser.py</code>)</p> <ul> <li>Responsibility: Interpreting source files and transforming them into a structured format.</li> <li>Details: This component contains parsers for each MedGen file format (e.g., <code>MRCONSO.RRF</code>, <code>NAMES.RRF</code>). It reads the raw, delimited text files and transforms each row into a structured Python <code>dataclass</code>. This module is designed to be a streaming iterator, which keeps memory usage low even for very large files. It also captures the raw source line for provenance.</li> </ul> </li> <li> <p>Loader (<code>loader/</code>)</p> <ul> <li>Responsibility: Loading the transformed data into the target database.</li> <li>Details: This is the most extensible part of the application. It uses a Factory pattern (<code>LoaderFactory</code>) and an Abstract Base Class (<code>AbstractNativeLoader</code>) to decouple the core logic from any specific database implementation. The default <code>PostgresNativeLoader</code> uses the highly efficient <code>COPY</code> command to bulk-load data from the parser's stream directly into staging tables. The loader is also responsible for managing database transactions to ensure atomicity.</li> </ul> </li> </ol>"},{"location":"architecture/#extensibility","title":"Extensibility","text":"<p>To add support for a new database (e.g., Redshift or BigQuery), a developer only needs to: 1.  Create a new loader class that inherits from <code>AbstractNativeLoader</code>. 2.  Implement the required methods (<code>initialize_staging</code>, <code>bulk_load</code>, <code>apply_changes</code>, etc.), making sure to use the target database's native bulk-loading mechanism. 3.  Register the new loader in the <code>LoaderFactory</code>.</p> <p>This design ensures that database-specific logic is encapsulated within its own module, keeping the rest of the codebase agnostic.</p>"},{"location":"cli/","title":"CLI Reference","text":"<p>The <code>py-load-medgen</code> tool is controlled via the command line.</p>"},{"location":"cli/#installation","title":"Installation","text":"<p>To use the tool, it is recommended to install it in a virtual environment. You can install it directly from the source code with the PostgreSQL driver:</p> <pre><code>pip install .[postgres]\n</code></pre> <p>To install for development, which includes testing and documentation dependencies:</p> <pre><code>pip install -e .[test,docs]\n</code></pre>"},{"location":"cli/#usage","title":"Usage","text":"<p>The main command is <code>py-load-medgen</code>.</p> <pre><code>py-load-medgen [OPTIONS]\n</code></pre>"},{"location":"cli/#options","title":"Options","text":"Option Type Default Environment Variable Description <code>--download-dir</code> <code>str</code> <code>.</code> The directory to download the MedGen files to. <code>--db-dsn</code> <code>str</code> <code>None</code> <code>MEDGEN_DB_DSN</code> The database connection string (DSN). Required. Can be set via this argument or the environment variable. <code>--mode</code> <code>str</code> <code>full</code> The load strategy: <code>full</code> for a complete refresh, or <code>delta</code> for incremental updates. <code>--max-parse-errors</code> <code>int</code> <code>100</code> The maximum number of parsing errors to tolerate before aborting the ETL process. <code>-h</code>, <code>--help</code> Show the help message and exit."},{"location":"cli/#examples","title":"Examples","text":"<p>Example 1: Full Load</p> <p>Perform a full refresh of the database, downloading files to <code>/tmp/medgen_data</code>. The DSN is provided by an environment variable.</p> <pre><code>export MEDGEN_DB_DSN=\"postgresql://user:password@host:port/dbname\"\npy-load-medgen --mode full --download-dir /tmp/medgen_data\n</code></pre> <p>Example 2: Delta Load with Low Error Tolerance</p> <p>Run an incremental update and abort if more than 10 parsing errors are found.</p> <pre><code>py-load-medgen --mode delta --max-parse-errors 10 --db-dsn \"...\"\n</code></pre>"},{"location":"reference/","title":"API Reference","text":"<p>This section provides an auto-generated API reference for the core modules in <code>py-load-medgen</code>.</p>"},{"location":"reference/#downloader","title":"Downloader","text":""},{"location":"reference/#py_load_medgen.downloader","title":"<code>py_load_medgen.downloader</code>","text":""},{"location":"reference/#py_load_medgen.downloader.Downloader","title":"<code>Downloader</code>","text":"<p>Handles downloading data files from the NCBI FTP server with retry logic.</p> Source code in <code>src/py_load_medgen/downloader.py</code> <pre><code>class Downloader:\n    \"\"\"\n    Handles downloading data files from the NCBI FTP server with retry logic.\n    \"\"\"\n\n    def __init__(self, ftp_host: str = \"ftp.ncbi.nlm.nih.gov\", ftp_path: str = \"/pub/medgen\"):\n        \"\"\"\n        Initializes the Downloader.\n        Args:\n            ftp_host: The hostname of the FTP server.\n            ftp_path: The base path to the MedGen files.\n        \"\"\"\n        self.ftp_host = ftp_host\n        self.ftp_path = ftp_path\n        self.ftp: Optional[ftplib.FTP] = None\n\n    def __enter__(self):\n        \"\"\"Establish the FTP connection.\"\"\"\n        logging.info(f\"Connecting to FTP server: {self.ftp_host}\")\n        self.ftp = ftplib.FTP(self.ftp_host)\n        self.ftp.login()  # Anonymous login\n        self.ftp.cwd(self.ftp_path)\n        logging.info(f\"Successfully connected and changed directory to {self.ftp_path}\")\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Close the FTP connection.\"\"\"\n        if self.ftp:\n            self.ftp.quit()\n            logging.info(\"FTP connection closed.\")\n\n    def list_files(self) -&gt; list[str]:\n        \"\"\"Lists files in the current FTP directory.\"\"\"\n        if not self.ftp:\n            raise ConnectionError(\"FTP connection not established.\")\n        return self.ftp.nlst()\n\n    def get_checksums(self, checksum_filename: str = \"md5sum.txt\") -&gt; dict[str, str]:\n        \"\"\"\n        Downloads and parses the checksum file.\n        Args:\n            checksum_filename: The name of the checksum file on the FTP server.\n        Returns:\n            A dictionary mapping filenames to their expected MD5 checksums.\n        \"\"\"\n        if not self.ftp:\n            raise ConnectionError(\"FTP connection not established.\")\n\n        checksums: dict[str, str] = {}\n        try:\n            lines: list[str] = []\n            self.ftp.retrlines(f\"RETR {checksum_filename}\", lines.append)\n            for line in lines:\n                # Format is typically: &lt;checksum&gt;  &lt;filename&gt;\n                parts = line.split()\n                if len(parts) == 2:\n                    checksum, filename = parts\n                    # The filenames in md5sum.txt might have a './' prefix\n                    checksums[filename.lstrip(\"./\")] = checksum\n            return checksums\n        except ftplib.all_errors as e:\n            logging.warning(f\"Could not find or parse checksum file '{checksum_filename}': {e}\")\n            return {} # Return empty dict if checksums aren't available\n\n    def get_release_version(self, readme_filename: str = \"README\") -&gt; str:\n        \"\"\"\n        Downloads the README file and parses it to find the release date/version.\n        Args:\n            readme_filename: The name of the README file on the FTP server.\n        Returns:\n            The release version string (e.g., a date) or \"Unknown\" if not found.\n        \"\"\"\n        if not self.ftp:\n            raise ConnectionError(\"FTP connection not established.\")\n\n        logging.info(f\"Attempting to find release version from '{readme_filename}'...\")\n        lines: list[str] = []\n        try:\n            self.ftp.retrlines(f\"RETR {readme_filename}\", lines.append)\n            for line in lines:\n                # Common patterns for release dates in README files\n                match = re.search(r\"(?:Last update|Release Date|Version):\\s*(.*)\", line, re.IGNORECASE)\n                if match:\n                    version = match.group(1).strip()\n                    logging.info(f\"Found release version: {version}\")\n                    return version\n\n            logging.warning(\"Release version not found in README. Returning 'Unknown'.\")\n            return \"Unknown\"\n        except ftplib.all_errors as e:\n            logging.warning(f\"Could not download or parse '{readme_filename}': {e}\")\n            return \"Unknown\"\n\n    @staticmethod\n    def _calculate_md5(filepath: Path) -&gt; str:\n        \"\"\"Calculates the MD5 checksum of a local file.\"\"\"\n        hash_md5 = hashlib.md5()\n        with open(filepath, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hash_md5.update(chunk)\n        return hash_md5.hexdigest()\n\n    def verify_file(self, local_filepath: Path, checksums: dict[str, str]) -&gt; bool:\n        \"\"\"\n        Verifies the integrity of a downloaded file using its MD5 checksum.\n        Args:\n            local_filepath: The path to the local file.\n            checksums: A dictionary of filenames and their expected checksums.\n        Returns:\n            True if the file is valid, False otherwise.\n        \"\"\"\n        filename = local_filepath.name\n        if filename not in checksums:\n            logging.warning(f\"No checksum found for {filename}. Skipping verification.\")\n            return True # Cannot verify, so assume it's okay for now.\n\n        expected_md5 = checksums[filename]\n        logging.info(f\"Verifying checksum for {filename}...\")\n        actual_md5 = self._calculate_md5(local_filepath)\n\n        if actual_md5 == expected_md5:\n            logging.info(f\"Checksum valid for {filename}.\")\n            return True\n        else:\n            logging.error(f\"Checksum mismatch for {filename}! Expected: {expected_md5}, Got: {actual_md5}\")\n            return False\n\n    @retry(\n        wait=wait_exponential(multiplier=1, min=2, max=60),\n        stop=stop_after_attempt(5),\n        reraise=True,\n    )\n    def download_file(\n        self, remote_filename: str, local_filepath: Path, checksums: Optional[dict[str, str]] = None\n    ) -&gt; None:\n        \"\"\"\n        Downloads a single file from the FTP server, resuming if partially downloaded,\n        and verifies its checksum upon completion.\n\n        Args:\n            remote_filename: The name of the file on the FTP server.\n            local_filepath: The local path to save the downloaded file.\n            checksums: A dictionary of checksums to verify against. If None, verification is skipped.\n        \"\"\"\n        if not self.ftp:\n            raise ConnectionError(\"FTP connection not established. Use within a 'with' statement.\")\n\n        try:\n            local_filepath.parent.mkdir(parents=True, exist_ok=True)\n\n            # --- Resumption Logic ---\n            rest_pos = 0\n            open_mode = \"wb\"\n            if local_filepath.exists():\n                rest_pos = local_filepath.stat().st_size\n                open_mode = \"ab\"\n\n            if rest_pos &gt; 0:\n                logging.info(\n                    f\"Resuming download for {remote_filename} from byte {rest_pos}.\"\n                )\n            else:\n                logging.info(f\"Downloading {remote_filename} to {local_filepath}...\")\n\n            # The 'rest' argument tells retrbinary where to start the download.\n            with open(local_filepath, open_mode) as f:\n                # The `rest` parameter is passed to the underlying `sendcmd`, so it should only\n                # be provided when we are actually resuming.\n                self.ftp.retrbinary(\n                    f\"RETR {remote_filename}\", f.write, rest=rest_pos if rest_pos &gt; 0 else None\n                )\n\n            logging.info(f\"Successfully downloaded {remote_filename}\")\n\n            if checksums:\n                if not self.verify_file(local_filepath, checksums):\n                    # If checksum fails, the file is corrupt. Delete it for a clean retry.\n                    local_filepath.unlink()\n                    raise ValueError(f\"Checksum validation failed for {remote_filename}\")\n\n        except (*ftplib.all_errors, ValueError) as e:\n            logging.error(f\"Error during download or verification of {remote_filename}: {e}\")\n            # If it's a checksum error, the file is already deleted.\n            # If it's an FTP error, we leave the partial file in place for the next retry attempt.\n            raise\n</code></pre>"},{"location":"reference/#py_load_medgen.downloader.Downloader.__enter__","title":"<code>__enter__()</code>","text":"<p>Establish the FTP connection.</p> Source code in <code>src/py_load_medgen/downloader.py</code> <pre><code>def __enter__(self):\n    \"\"\"Establish the FTP connection.\"\"\"\n    logging.info(f\"Connecting to FTP server: {self.ftp_host}\")\n    self.ftp = ftplib.FTP(self.ftp_host)\n    self.ftp.login()  # Anonymous login\n    self.ftp.cwd(self.ftp_path)\n    logging.info(f\"Successfully connected and changed directory to {self.ftp_path}\")\n    return self\n</code></pre>"},{"location":"reference/#py_load_medgen.downloader.Downloader.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Close the FTP connection.</p> Source code in <code>src/py_load_medgen/downloader.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Close the FTP connection.\"\"\"\n    if self.ftp:\n        self.ftp.quit()\n        logging.info(\"FTP connection closed.\")\n</code></pre>"},{"location":"reference/#py_load_medgen.downloader.Downloader.__init__","title":"<code>__init__(ftp_host='ftp.ncbi.nlm.nih.gov', ftp_path='/pub/medgen')</code>","text":"<p>Initializes the Downloader. Args:     ftp_host: The hostname of the FTP server.     ftp_path: The base path to the MedGen files.</p> Source code in <code>src/py_load_medgen/downloader.py</code> <pre><code>def __init__(self, ftp_host: str = \"ftp.ncbi.nlm.nih.gov\", ftp_path: str = \"/pub/medgen\"):\n    \"\"\"\n    Initializes the Downloader.\n    Args:\n        ftp_host: The hostname of the FTP server.\n        ftp_path: The base path to the MedGen files.\n    \"\"\"\n    self.ftp_host = ftp_host\n    self.ftp_path = ftp_path\n    self.ftp: Optional[ftplib.FTP] = None\n</code></pre>"},{"location":"reference/#py_load_medgen.downloader.Downloader.download_file","title":"<code>download_file(remote_filename, local_filepath, checksums=None)</code>","text":"<p>Downloads a single file from the FTP server, resuming if partially downloaded, and verifies its checksum upon completion.</p> <p>Parameters:</p> Name Type Description Default <code>remote_filename</code> <code>str</code> <p>The name of the file on the FTP server.</p> required <code>local_filepath</code> <code>Path</code> <p>The local path to save the downloaded file.</p> required <code>checksums</code> <code>Optional[dict[str, str]]</code> <p>A dictionary of checksums to verify against. If None, verification is skipped.</p> <code>None</code> Source code in <code>src/py_load_medgen/downloader.py</code> <pre><code>@retry(\n    wait=wait_exponential(multiplier=1, min=2, max=60),\n    stop=stop_after_attempt(5),\n    reraise=True,\n)\ndef download_file(\n    self, remote_filename: str, local_filepath: Path, checksums: Optional[dict[str, str]] = None\n) -&gt; None:\n    \"\"\"\n    Downloads a single file from the FTP server, resuming if partially downloaded,\n    and verifies its checksum upon completion.\n\n    Args:\n        remote_filename: The name of the file on the FTP server.\n        local_filepath: The local path to save the downloaded file.\n        checksums: A dictionary of checksums to verify against. If None, verification is skipped.\n    \"\"\"\n    if not self.ftp:\n        raise ConnectionError(\"FTP connection not established. Use within a 'with' statement.\")\n\n    try:\n        local_filepath.parent.mkdir(parents=True, exist_ok=True)\n\n        # --- Resumption Logic ---\n        rest_pos = 0\n        open_mode = \"wb\"\n        if local_filepath.exists():\n            rest_pos = local_filepath.stat().st_size\n            open_mode = \"ab\"\n\n        if rest_pos &gt; 0:\n            logging.info(\n                f\"Resuming download for {remote_filename} from byte {rest_pos}.\"\n            )\n        else:\n            logging.info(f\"Downloading {remote_filename} to {local_filepath}...\")\n\n        # The 'rest' argument tells retrbinary where to start the download.\n        with open(local_filepath, open_mode) as f:\n            # The `rest` parameter is passed to the underlying `sendcmd`, so it should only\n            # be provided when we are actually resuming.\n            self.ftp.retrbinary(\n                f\"RETR {remote_filename}\", f.write, rest=rest_pos if rest_pos &gt; 0 else None\n            )\n\n        logging.info(f\"Successfully downloaded {remote_filename}\")\n\n        if checksums:\n            if not self.verify_file(local_filepath, checksums):\n                # If checksum fails, the file is corrupt. Delete it for a clean retry.\n                local_filepath.unlink()\n                raise ValueError(f\"Checksum validation failed for {remote_filename}\")\n\n    except (*ftplib.all_errors, ValueError) as e:\n        logging.error(f\"Error during download or verification of {remote_filename}: {e}\")\n        # If it's a checksum error, the file is already deleted.\n        # If it's an FTP error, we leave the partial file in place for the next retry attempt.\n        raise\n</code></pre>"},{"location":"reference/#py_load_medgen.downloader.Downloader.get_checksums","title":"<code>get_checksums(checksum_filename='md5sum.txt')</code>","text":"<p>Downloads and parses the checksum file. Args:     checksum_filename: The name of the checksum file on the FTP server. Returns:     A dictionary mapping filenames to their expected MD5 checksums.</p> Source code in <code>src/py_load_medgen/downloader.py</code> <pre><code>def get_checksums(self, checksum_filename: str = \"md5sum.txt\") -&gt; dict[str, str]:\n    \"\"\"\n    Downloads and parses the checksum file.\n    Args:\n        checksum_filename: The name of the checksum file on the FTP server.\n    Returns:\n        A dictionary mapping filenames to their expected MD5 checksums.\n    \"\"\"\n    if not self.ftp:\n        raise ConnectionError(\"FTP connection not established.\")\n\n    checksums: dict[str, str] = {}\n    try:\n        lines: list[str] = []\n        self.ftp.retrlines(f\"RETR {checksum_filename}\", lines.append)\n        for line in lines:\n            # Format is typically: &lt;checksum&gt;  &lt;filename&gt;\n            parts = line.split()\n            if len(parts) == 2:\n                checksum, filename = parts\n                # The filenames in md5sum.txt might have a './' prefix\n                checksums[filename.lstrip(\"./\")] = checksum\n        return checksums\n    except ftplib.all_errors as e:\n        logging.warning(f\"Could not find or parse checksum file '{checksum_filename}': {e}\")\n        return {} # Return empty dict if checksums aren't available\n</code></pre>"},{"location":"reference/#py_load_medgen.downloader.Downloader.get_release_version","title":"<code>get_release_version(readme_filename='README')</code>","text":"<p>Downloads the README file and parses it to find the release date/version. Args:     readme_filename: The name of the README file on the FTP server. Returns:     The release version string (e.g., a date) or \"Unknown\" if not found.</p> Source code in <code>src/py_load_medgen/downloader.py</code> <pre><code>def get_release_version(self, readme_filename: str = \"README\") -&gt; str:\n    \"\"\"\n    Downloads the README file and parses it to find the release date/version.\n    Args:\n        readme_filename: The name of the README file on the FTP server.\n    Returns:\n        The release version string (e.g., a date) or \"Unknown\" if not found.\n    \"\"\"\n    if not self.ftp:\n        raise ConnectionError(\"FTP connection not established.\")\n\n    logging.info(f\"Attempting to find release version from '{readme_filename}'...\")\n    lines: list[str] = []\n    try:\n        self.ftp.retrlines(f\"RETR {readme_filename}\", lines.append)\n        for line in lines:\n            # Common patterns for release dates in README files\n            match = re.search(r\"(?:Last update|Release Date|Version):\\s*(.*)\", line, re.IGNORECASE)\n            if match:\n                version = match.group(1).strip()\n                logging.info(f\"Found release version: {version}\")\n                return version\n\n        logging.warning(\"Release version not found in README. Returning 'Unknown'.\")\n        return \"Unknown\"\n    except ftplib.all_errors as e:\n        logging.warning(f\"Could not download or parse '{readme_filename}': {e}\")\n        return \"Unknown\"\n</code></pre>"},{"location":"reference/#py_load_medgen.downloader.Downloader.list_files","title":"<code>list_files()</code>","text":"<p>Lists files in the current FTP directory.</p> Source code in <code>src/py_load_medgen/downloader.py</code> <pre><code>def list_files(self) -&gt; list[str]:\n    \"\"\"Lists files in the current FTP directory.\"\"\"\n    if not self.ftp:\n        raise ConnectionError(\"FTP connection not established.\")\n    return self.ftp.nlst()\n</code></pre>"},{"location":"reference/#py_load_medgen.downloader.Downloader.verify_file","title":"<code>verify_file(local_filepath, checksums)</code>","text":"<p>Verifies the integrity of a downloaded file using its MD5 checksum. Args:     local_filepath: The path to the local file.     checksums: A dictionary of filenames and their expected checksums. Returns:     True if the file is valid, False otherwise.</p> Source code in <code>src/py_load_medgen/downloader.py</code> <pre><code>def verify_file(self, local_filepath: Path, checksums: dict[str, str]) -&gt; bool:\n    \"\"\"\n    Verifies the integrity of a downloaded file using its MD5 checksum.\n    Args:\n        local_filepath: The path to the local file.\n        checksums: A dictionary of filenames and their expected checksums.\n    Returns:\n        True if the file is valid, False otherwise.\n    \"\"\"\n    filename = local_filepath.name\n    if filename not in checksums:\n        logging.warning(f\"No checksum found for {filename}. Skipping verification.\")\n        return True # Cannot verify, so assume it's okay for now.\n\n    expected_md5 = checksums[filename]\n    logging.info(f\"Verifying checksum for {filename}...\")\n    actual_md5 = self._calculate_md5(local_filepath)\n\n    if actual_md5 == expected_md5:\n        logging.info(f\"Checksum valid for {filename}.\")\n        return True\n    else:\n        logging.error(f\"Checksum mismatch for {filename}! Expected: {expected_md5}, Got: {actual_md5}\")\n        return False\n</code></pre>"},{"location":"reference/#parser","title":"Parser","text":""},{"location":"reference/#py_load_medgen.parser","title":"<code>py_load_medgen.parser</code>","text":""},{"location":"reference/#py_load_medgen.parser.MedgenHpoMapping","title":"<code>MedgenHpoMapping</code>  <code>dataclass</code>","text":"<p>Represents a single record from the MedGen_HPO_Mapping.txt.gz file.</p> Source code in <code>src/py_load_medgen/parser.py</code> <pre><code>@dataclass(frozen=True)\nclass MedgenHpoMapping:\n    \"\"\"Represents a single record from the MedGen_HPO_Mapping.txt.gz file.\"\"\"\n\n    cui: str\n    sdui: str\n    hpo_str: str\n    medgen_str: str\n    medgen_str_sab: str\n    sty: str\n    raw_record: str\n</code></pre>"},{"location":"reference/#py_load_medgen.parser.MedgenName","title":"<code>MedgenName</code>  <code>dataclass</code>","text":"<p>Represents a single record from the NAMES.RRF file.</p> Source code in <code>src/py_load_medgen/parser.py</code> <pre><code>@dataclass(frozen=True)\nclass MedgenName:\n    \"\"\"Represents a single record from the NAMES.RRF file.\"\"\"\n\n    cui: str\n    name: str\n    source: str\n    suppress: str\n    raw_record: str\n</code></pre>"},{"location":"reference/#py_load_medgen.parser.MrconsoRecord","title":"<code>MrconsoRecord</code>  <code>dataclass</code>","text":"<p>Represents a single record from the MRCONSO.RRF file. Field names correspond to the columns defined in the UMLS Reference Manual. See: https://www.ncbi.nlm.nih.gov/books/NBK9685/</p> Source code in <code>src/py_load_medgen/parser.py</code> <pre><code>@dataclass(frozen=True)\nclass MrconsoRecord:\n    \"\"\"\n    Represents a single record from the MRCONSO.RRF file.\n    Field names correspond to the columns defined in the UMLS Reference Manual.\n    See: https://www.ncbi.nlm.nih.gov/books/NBK9685/\n    \"\"\"\n\n    cui: str\n    lat: str\n    ts: str\n    lui: str\n    stt: str\n    sui: str\n    ispref: str\n    aui: str\n    saui: Optional[str]\n    scui: Optional[str]\n    sdui: Optional[str]\n    sab: str\n    tty: str\n    code: str\n    record_str: str\n    srl: str\n    suppress: str\n    cvf: Optional[str]\n    raw_record: str\n</code></pre>"},{"location":"reference/#py_load_medgen.parser.MrrelRecord","title":"<code>MrrelRecord</code>  <code>dataclass</code>","text":"<p>Represents a single record from the MRREL.RRF file. Field names correspond to the columns defined in the UMLS Reference Manual. See: https://www.ncbi.nlm.nih.gov/books/NBK9685/table/ch03.T.related_concepts_file_mrrel_rrf/</p> Source code in <code>src/py_load_medgen/parser.py</code> <pre><code>@dataclass(frozen=True)\nclass MrrelRecord:\n    \"\"\"\n    Represents a single record from the MRREL.RRF file.\n    Field names correspond to the columns defined in the UMLS Reference Manual.\n    See: https://www.ncbi.nlm.nih.gov/books/NBK9685/table/ch03.T.related_concepts_file_mrrel_rrf/\n    \"\"\"\n\n    cui1: str\n    aui1: Optional[str]\n    stype1: str\n    rel: str\n    cui2: str\n    aui2: Optional[str]\n    stype2: str\n    rela: Optional[str]\n    rui: Optional[str]\n    srui: Optional[str]\n    sab: str\n    sl: Optional[str]\n    rg: Optional[str]\n    dir: Optional[str]\n    suppress: str\n    cvf: Optional[str]\n    raw_record: str\n</code></pre>"},{"location":"reference/#py_load_medgen.parser.MrstyRecord","title":"<code>MrstyRecord</code>  <code>dataclass</code>","text":"<p>Represents a single record from the MRSTY.RRF file. Field names correspond to the columns defined in the UMLS Reference Manual. See: https://www.ncbi.nlm.nih.gov/books/NBK9685/table/ch03.Tf/</p> Source code in <code>src/py_load_medgen/parser.py</code> <pre><code>@dataclass(frozen=True)\nclass MrstyRecord:\n    \"\"\"\n    Represents a single record from the MRSTY.RRF file.\n    Field names correspond to the columns defined in the UMLS Reference Manual.\n    See: https://www.ncbi.nlm.nih.gov/books/NBK9685/table/ch03.Tf/\n    \"\"\"\n\n    cui: str\n    tui: str\n    stn: str\n    sty: str\n    atui: Optional[str]\n    cvf: Optional[str]\n    raw_record: str\n</code></pre>"},{"location":"reference/#py_load_medgen.parser.parse_hpo_mapping","title":"<code>parse_hpo_mapping(file_path, max_errors)</code>","text":"<p>Parses a gzipped, tab-delimited MedGen_HPO_Mapping.txt.gz file. Args:     file_path: Path to the gzipped file.     max_errors: The maximum number of parsing errors to tolerate. Yields:     MedgenHpoMapping instances for each valid row in the file. Raises:     ValueError: If the number of parsing errors exceeds max_errors.</p> Source code in <code>src/py_load_medgen/parser.py</code> <pre><code>def parse_hpo_mapping(file_path: Path, max_errors: int) -&gt; Iterator[MedgenHpoMapping]:\n    \"\"\"\n    Parses a gzipped, tab-delimited MedGen_HPO_Mapping.txt.gz file.\n    Args:\n        file_path: Path to the gzipped file.\n        max_errors: The maximum number of parsing errors to tolerate.\n    Yields:\n        MedgenHpoMapping instances for each valid row in the file.\n    Raises:\n        ValueError: If the number of parsing errors exceeds max_errors.\n    \"\"\"\n    error_count = 0\n    with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n        first_line = f.readline()\n        if not first_line.lower().startswith(\"#cui\") and not first_line.lower().startswith(\"cui\"):\n            f.seek(0)\n\n        for i, line in enumerate(f):\n            raw_line = line.strip()\n            if not raw_line:\n                continue\n\n            row = raw_line.split(\"\\t\")\n            if len(row) != 6:\n                error_count += 1\n                logging.warning(\n                    f\"Skipping malformed row {i+1} in HPO Mapping file: \"\n                    f\"expected 6 columns, found {len(row)}\"\n                )\n                if error_count &gt; max_errors:\n                    raise ValueError(f\"Exceeded maximum parsing errors ({max_errors}). Aborting.\")\n                continue\n\n            yield MedgenHpoMapping(\n                cui=row[0],\n                sdui=row[1],\n                hpo_str=row[2],\n                medgen_str=row[3],\n                medgen_str_sab=row[4],\n                sty=row[5],\n                raw_record=raw_line,\n            )\n</code></pre>"},{"location":"reference/#py_load_medgen.parser.parse_mrconso","title":"<code>parse_mrconso(file_stream, max_errors)</code>","text":"<p>Parses a pipe-delimited MRCONSO.RRF file stream. Args:     file_stream: A text file-like object containing MRCONSO.RRF data.     max_errors: The maximum number of parsing errors to tolerate. Yields:     MrconsoRecord instances for each valid row in the file. Raises:     ValueError: If the number of parsing errors exceeds max_errors.</p> Source code in <code>src/py_load_medgen/parser.py</code> <pre><code>def parse_mrconso(file_stream: IO[str], max_errors: int) -&gt; Iterator[MrconsoRecord]:\n    \"\"\"\n    Parses a pipe-delimited MRCONSO.RRF file stream.\n    Args:\n        file_stream: A text file-like object containing MRCONSO.RRF data.\n        max_errors: The maximum number of parsing errors to tolerate.\n    Yields:\n        MrconsoRecord instances for each valid row in the file.\n    Raises:\n        ValueError: If the number of parsing errors exceeds max_errors.\n    \"\"\"\n    error_count = 0\n    for i, line in enumerate(file_stream):\n        raw_line = line.strip()\n        if not raw_line:\n            continue\n\n        row = raw_line.split(\"|\")\n        if len(row) &lt; 18:\n            error_count += 1\n            logging.warning(\n                f\"Skipping malformed row {i+1}: expected 18 columns, found {len(row) - 1}\"\n            )\n            if error_count &gt; max_errors:\n                raise ValueError(f\"Exceeded maximum parsing errors ({max_errors}). Aborting.\")\n            continue\n\n        try:\n            yield MrconsoRecord(\n                cui=row[0],\n                lat=row[1],\n                ts=row[2],\n                lui=row[3],\n                stt=row[4],\n                sui=row[5],\n                ispref=row[6],\n                aui=row[7],\n                saui=row[8] if row[8] else None,\n                scui=row[9] if row[9] else None,\n                sdui=row[10] if row[10] else None,\n                sab=row[11],\n                tty=row[12],\n                code=row[13],\n                record_str=row[14],\n                srl=row[15],\n                suppress=row[16],\n                cvf=row[17] if row[17] else None,\n                raw_record=raw_line,\n            )\n        except IndexError:\n            error_count += 1\n            logging.warning(f\"Skipping malformed row {i+1}: not enough columns.\")\n            if error_count &gt; max_errors:\n                raise ValueError(f\"Exceeded maximum parsing errors ({max_errors}). Aborting.\")\n</code></pre>"},{"location":"reference/#py_load_medgen.parser.parse_mrrel","title":"<code>parse_mrrel(file_stream, max_errors)</code>","text":"<p>Parses a pipe-delimited MRREL.RRF file stream. Args:     file_stream: A text file-like object containing MRREL.RRF data.     max_errors: The maximum number of parsing errors to tolerate. Yields:     MrrelRecord instances for each valid row in the file. Raises:     ValueError: If the number of parsing errors exceeds max_errors.</p> Source code in <code>src/py_load_medgen/parser.py</code> <pre><code>def parse_mrrel(file_stream: IO[str], max_errors: int) -&gt; Iterator[MrrelRecord]:\n    \"\"\"\n    Parses a pipe-delimited MRREL.RRF file stream.\n    Args:\n        file_stream: A text file-like object containing MRREL.RRF data.\n        max_errors: The maximum number of parsing errors to tolerate.\n    Yields:\n        MrrelRecord instances for each valid row in the file.\n    Raises:\n        ValueError: If the number of parsing errors exceeds max_errors.\n    \"\"\"\n    error_count = 0\n    for i, line in enumerate(file_stream):\n        raw_line = line.strip()\n        if not raw_line:\n            continue\n\n        row = raw_line.split(\"|\")\n        if len(row) &lt; 16:\n            error_count += 1\n            logging.warning(\n                f\"Skipping malformed row {i+1} in MRREL.RRF: expected 16 columns, found {len(row) - 1}\"\n            )\n            if error_count &gt; max_errors:\n                raise ValueError(f\"Exceeded maximum parsing errors ({max_errors}). Aborting.\")\n            continue\n\n        try:\n            yield MrrelRecord(\n                cui1=row[0],\n                aui1=row[1] if row[1] else None,\n                stype1=row[2],\n                rel=row[3],\n                cui2=row[4],\n                aui2=row[5] if row[5] else None,\n                stype2=row[6],\n                rela=row[7] if row[7] else None,\n                rui=row[8] if row[8] else None,\n                srui=row[9] if row[9] else None,\n                sab=row[10],\n                sl=row[11] if row[11] else None,\n                rg=row[12] if row[12] else None,\n                dir=row[13] if row[13] else None,\n                suppress=row[14],\n                cvf=row[15] if row[15] else None,\n                raw_record=raw_line,\n            )\n        except IndexError:\n            error_count += 1\n            logging.warning(f\"Skipping malformed row {i+1} in MRREL.RRF: not enough columns.\")\n            if error_count &gt; max_errors:\n                raise ValueError(f\"Exceeded maximum parsing errors ({max_errors}). Aborting.\")\n</code></pre>"},{"location":"reference/#py_load_medgen.parser.parse_mrsty","title":"<code>parse_mrsty(file_stream, max_errors)</code>","text":"<p>Parses a pipe-delimited MRSTY.RRF file stream. Args:     file_stream: A text file-like object containing MRSTY.RRF data.     max_errors: The maximum number of parsing errors to tolerate. Yields:     MrstyRecord instances for each valid row in the file. Raises:     ValueError: If the number of parsing errors exceeds max_errors.</p> Source code in <code>src/py_load_medgen/parser.py</code> <pre><code>def parse_mrsty(file_stream: IO[str], max_errors: int) -&gt; Iterator[MrstyRecord]:\n    \"\"\"\n    Parses a pipe-delimited MRSTY.RRF file stream.\n    Args:\n        file_stream: A text file-like object containing MRSTY.RRF data.\n        max_errors: The maximum number of parsing errors to tolerate.\n    Yields:\n        MrstyRecord instances for each valid row in the file.\n    Raises:\n        ValueError: If the number of parsing errors exceeds max_errors.\n    \"\"\"\n    error_count = 0\n    for i, line in enumerate(file_stream):\n        raw_line = line.strip()\n        if not raw_line:\n            continue\n\n        row = raw_line.split(\"|\")\n        if len(row) &lt; 6:\n            error_count += 1\n            logging.warning(\n                f\"Skipping malformed row {i+1} in MRSTY.RRF: expected 6 columns, found {len(row) - 1}\"\n            )\n            if error_count &gt; max_errors:\n                raise ValueError(f\"Exceeded maximum parsing errors ({max_errors}). Aborting.\")\n            continue\n\n        try:\n            yield MrstyRecord(\n                cui=row[0],\n                tui=row[1],\n                stn=row[2],\n                sty=row[3],\n                atui=row[4] if row[4] else None,\n                cvf=row[5] if row[5] else None,\n                raw_record=raw_line,\n            )\n        except IndexError:\n            error_count += 1\n            logging.warning(f\"Skipping malformed row {i+1} in MRSTY.RRF: not enough columns.\")\n            if error_count &gt; max_errors:\n                raise ValueError(f\"Exceeded maximum parsing errors ({max_errors}). Aborting.\")\n</code></pre>"},{"location":"reference/#py_load_medgen.parser.parse_names","title":"<code>parse_names(file_path, max_errors)</code>","text":"<p>Parses a gzipped, pipe-delimited NAMES.RRF.gz file. Args:     file_path: Path to the gzipped file.     max_errors: The maximum number of parsing errors to tolerate. Yields:     MedgenName instances for each valid row in the file. Raises:     ValueError: If the number of parsing errors exceeds max_errors.</p> Source code in <code>src/py_load_medgen/parser.py</code> <pre><code>def parse_names(file_path: Path, max_errors: int) -&gt; Iterator[MedgenName]:\n    \"\"\"\n    Parses a gzipped, pipe-delimited NAMES.RRF.gz file.\n    Args:\n        file_path: Path to the gzipped file.\n        max_errors: The maximum number of parsing errors to tolerate.\n    Yields:\n        MedgenName instances for each valid row in the file.\n    Raises:\n        ValueError: If the number of parsing errors exceeds max_errors.\n    \"\"\"\n    error_count = 0\n    with gzip.open(file_path, \"rt\", encoding=\"utf-8\") as f:\n        header = f.readline()\n        if not header.startswith(\"#CUI\"):\n            logging.warning(\"NAMES.RRF file does not have the expected header.\")\n\n        for i, line in enumerate(f):\n            raw_line = line.strip()\n            if not raw_line:\n                continue\n\n            row = raw_line.split(\"|\")\n            if len(row) &lt; 5:\n                error_count += 1\n                logging.warning(\n                    f\"Skipping malformed row {i+1} in NAMES.RRF: expected 4 columns, found {len(row) - 1}\"\n                )\n                if error_count &gt; max_errors:\n                    raise ValueError(f\"Exceeded maximum parsing errors ({max_errors}). Aborting.\")\n                continue\n\n            yield MedgenName(\n                cui=row[0],\n                name=row[1],\n                source=row[2],\n                suppress=row[3],\n                raw_record=raw_line,\n            )\n</code></pre>"},{"location":"reference/#py_load_medgen.parser.stream_hpo_mapping_tsv","title":"<code>stream_hpo_mapping_tsv(records)</code>","text":"<p>Transforms an iterator of MedgenHpoMapping objects into a streaming iterator of UTF-8 encoded TSV lines.</p> Source code in <code>src/py_load_medgen/parser.py</code> <pre><code>def stream_hpo_mapping_tsv(records: Iterator[MedgenHpoMapping]) -&gt; Iterator[bytes]:\n    \"\"\"\n    Transforms an iterator of MedgenHpoMapping objects into a streaming iterator\n    of UTF-8 encoded TSV lines.\n    \"\"\"\n    for record in records:\n        line = \"\\t\".join(\n            str(getattr(record, field.name) or r\"\\N\") for field in fields(record)\n        )\n        yield (line + \"\\n\").encode(\"utf-8\")\n</code></pre>"},{"location":"reference/#py_load_medgen.parser.stream_mrconso_tsv","title":"<code>stream_mrconso_tsv(records)</code>","text":"<p>Transforms an iterator of MrconsoRecord objects into a streaming iterator of UTF-8 encoded TSV lines.</p> Source code in <code>src/py_load_medgen/parser.py</code> <pre><code>def stream_mrconso_tsv(records: Iterator[MrconsoRecord]) -&gt; Iterator[bytes]:\n    \"\"\"\n    Transforms an iterator of MrconsoRecord objects into a streaming iterator\n    of UTF-8 encoded TSV lines.\n    \"\"\"\n    for record in records:\n        line = \"\\t\".join(\n            str(getattr(record, field.name) or r\"\\N\") for field in fields(record)\n        )\n        yield (line + \"\\n\").encode(\"utf-8\")\n</code></pre>"},{"location":"reference/#py_load_medgen.parser.stream_mrrel_tsv","title":"<code>stream_mrrel_tsv(records)</code>","text":"<p>Transforms an iterator of MrrelRecord objects into a streaming iterator of UTF-8 encoded TSV lines.</p> Source code in <code>src/py_load_medgen/parser.py</code> <pre><code>def stream_mrrel_tsv(records: Iterator[MrrelRecord]) -&gt; Iterator[bytes]:\n    \"\"\"\n    Transforms an iterator of MrrelRecord objects into a streaming iterator\n    of UTF-8 encoded TSV lines.\n    \"\"\"\n    for record in records:\n        line = \"\\t\".join(\n            str(getattr(record, field.name) or r\"\\N\") for field in fields(record)\n        )\n        yield (line + \"\\n\").encode(\"utf-8\")\n</code></pre>"},{"location":"reference/#py_load_medgen.parser.stream_mrsty_tsv","title":"<code>stream_mrsty_tsv(records)</code>","text":"<p>Transforms an iterator of MrstyRecord objects into a streaming iterator of UTF-8 encoded TSV lines.</p> Source code in <code>src/py_load_medgen/parser.py</code> <pre><code>def stream_mrsty_tsv(records: Iterator[MrstyRecord]) -&gt; Iterator[bytes]:\n    \"\"\"\n    Transforms an iterator of MrstyRecord objects into a streaming iterator\n    of UTF-8 encoded TSV lines.\n    \"\"\"\n    for record in records:\n        line = \"\\t\".join(\n            str(getattr(record, field.name) or r\"\\N\") for field in fields(record)\n        )\n        yield (line + \"\\n\").encode(\"utf-8\")\n</code></pre>"},{"location":"reference/#py_load_medgen.parser.stream_names_tsv","title":"<code>stream_names_tsv(records)</code>","text":"<p>Transforms an iterator of MedgenName objects into a streaming iterator of UTF-8 encoded TSV lines.</p> Source code in <code>src/py_load_medgen/parser.py</code> <pre><code>def stream_names_tsv(records: Iterator[MedgenName]) -&gt; Iterator[bytes]:\n    \"\"\"\n    Transforms an iterator of MedgenName objects into a streaming iterator\n    of UTF-8 encoded TSV lines.\n    \"\"\"\n    for record in records:\n        line = \"\\t\".join(\n            str(getattr(record, field.name) or r\"\\N\") for field in fields(record)\n        )\n        yield (line + \"\\n\").encode(\"utf-8\")\n</code></pre>"},{"location":"reference/#loader-interface","title":"Loader Interface","text":"<p>This section covers the abstract base classes and factories for the loader module.</p>"},{"location":"reference/#py_load_medgen.loader.base","title":"<code>py_load_medgen.loader.base</code>","text":""},{"location":"reference/#py_load_medgen.loader.base.AbstractNativeLoader","title":"<code>AbstractNativeLoader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract Base Class for database-specific native bulk loaders. This class defines the standard interface that all native loaders must implement to support the ETL process.</p> Source code in <code>src/py_load_medgen/loader/base.py</code> <pre><code>class AbstractNativeLoader(ABC):\n    \"\"\"\n    Abstract Base Class for database-specific native bulk loaders.\n    This class defines the standard interface that all native loaders must\n    implement to support the ETL process.\n    \"\"\"\n\n    @abstractmethod\n    def connect(self) -&gt; None:\n        \"\"\"Establishes a connection to the target database.\"\"\"\n        raise NotImplementedError\n\n    def __enter__(self):\n        \"\"\"Context manager entry point, establishes connection.\"\"\"\n        self.connect()\n        return self\n\n    @abstractmethod\n    def close(self) -&gt; None:\n        \"\"\"Closes the database connection.\"\"\"\n        raise NotImplementedError\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit point, closes connection.\"\"\"\n        self.close()\n\n    @abstractmethod\n    def initialize_staging(self, table_name: str, ddl: str) -&gt; None:\n        \"\"\"Creates and prepares a single staging table for data loading.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def bulk_load(self, table_name: str, data_iterator: Iterator[bytes]) -&gt; None:\n        \"\"\"\n        Executes a native, high-performance bulk load operation.\n        Args:\n            table_name: The name of the target staging table.\n            data_iterator: An iterator yielding bytes (e.g., TSV lines).\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def execute_cdc(\n        self, staging_table: str, production_table: str, pk_name: str, business_key: str\n    ) -&gt; dict[str, int]:\n        \"\"\"\n        Executes the Change Data Capture (CDC) logic to identify inserts,\n        updates, and deletes by comparing staging and production data.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def apply_changes(\n        self,\n        mode: str,\n        staging_table: str,\n        production_table: str,\n        production_ddl: str,\n        index_ddls: list[str],\n        pk_name: str,\n        business_key: str,\n    ) -&gt; None:\n        \"\"\"\n        Applies the identified changes (inserts, updates, deletes) to the\n        production tables atomically. This could involve a table swap or\n        merge operations.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def cleanup(self, staging_table: str, production_table: str) -&gt; None:\n        \"\"\"Performs cleanup operations, such as dropping staging tables.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def log_run_start(\n        self, run_id: uuid.UUID, package_version: str, load_mode: str, source_files: dict\n    ) -&gt; int:\n        \"\"\"Logs the start of an ETL run and returns a unique identifier for the run log.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def log_run_finish(\n        self,\n        log_id: int,\n        status: str,\n        records_extracted: int,\n        records_loaded: int,\n        error_message: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Logs the completion or failure of an ETL run.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.base.AbstractNativeLoader.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry point, establishes connection.</p> Source code in <code>src/py_load_medgen/loader/base.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry point, establishes connection.\"\"\"\n    self.connect()\n    return self\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.base.AbstractNativeLoader.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit point, closes connection.</p> Source code in <code>src/py_load_medgen/loader/base.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit point, closes connection.\"\"\"\n    self.close()\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.base.AbstractNativeLoader.apply_changes","title":"<code>apply_changes(mode, staging_table, production_table, production_ddl, index_ddls, pk_name, business_key)</code>  <code>abstractmethod</code>","text":"<p>Applies the identified changes (inserts, updates, deletes) to the production tables atomically. This could involve a table swap or merge operations.</p> Source code in <code>src/py_load_medgen/loader/base.py</code> <pre><code>@abstractmethod\ndef apply_changes(\n    self,\n    mode: str,\n    staging_table: str,\n    production_table: str,\n    production_ddl: str,\n    index_ddls: list[str],\n    pk_name: str,\n    business_key: str,\n) -&gt; None:\n    \"\"\"\n    Applies the identified changes (inserts, updates, deletes) to the\n    production tables atomically. This could involve a table swap or\n    merge operations.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.base.AbstractNativeLoader.bulk_load","title":"<code>bulk_load(table_name, data_iterator)</code>  <code>abstractmethod</code>","text":"<p>Executes a native, high-performance bulk load operation. Args:     table_name: The name of the target staging table.     data_iterator: An iterator yielding bytes (e.g., TSV lines).</p> Source code in <code>src/py_load_medgen/loader/base.py</code> <pre><code>@abstractmethod\ndef bulk_load(self, table_name: str, data_iterator: Iterator[bytes]) -&gt; None:\n    \"\"\"\n    Executes a native, high-performance bulk load operation.\n    Args:\n        table_name: The name of the target staging table.\n        data_iterator: An iterator yielding bytes (e.g., TSV lines).\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.base.AbstractNativeLoader.cleanup","title":"<code>cleanup(staging_table, production_table)</code>  <code>abstractmethod</code>","text":"<p>Performs cleanup operations, such as dropping staging tables.</p> Source code in <code>src/py_load_medgen/loader/base.py</code> <pre><code>@abstractmethod\ndef cleanup(self, staging_table: str, production_table: str) -&gt; None:\n    \"\"\"Performs cleanup operations, such as dropping staging tables.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.base.AbstractNativeLoader.close","title":"<code>close()</code>  <code>abstractmethod</code>","text":"<p>Closes the database connection.</p> Source code in <code>src/py_load_medgen/loader/base.py</code> <pre><code>@abstractmethod\ndef close(self) -&gt; None:\n    \"\"\"Closes the database connection.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.base.AbstractNativeLoader.connect","title":"<code>connect()</code>  <code>abstractmethod</code>","text":"<p>Establishes a connection to the target database.</p> Source code in <code>src/py_load_medgen/loader/base.py</code> <pre><code>@abstractmethod\ndef connect(self) -&gt; None:\n    \"\"\"Establishes a connection to the target database.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.base.AbstractNativeLoader.execute_cdc","title":"<code>execute_cdc(staging_table, production_table, pk_name, business_key)</code>  <code>abstractmethod</code>","text":"<p>Executes the Change Data Capture (CDC) logic to identify inserts, updates, and deletes by comparing staging and production data.</p> Source code in <code>src/py_load_medgen/loader/base.py</code> <pre><code>@abstractmethod\ndef execute_cdc(\n    self, staging_table: str, production_table: str, pk_name: str, business_key: str\n) -&gt; dict[str, int]:\n    \"\"\"\n    Executes the Change Data Capture (CDC) logic to identify inserts,\n    updates, and deletes by comparing staging and production data.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.base.AbstractNativeLoader.initialize_staging","title":"<code>initialize_staging(table_name, ddl)</code>  <code>abstractmethod</code>","text":"<p>Creates and prepares a single staging table for data loading.</p> Source code in <code>src/py_load_medgen/loader/base.py</code> <pre><code>@abstractmethod\ndef initialize_staging(self, table_name: str, ddl: str) -&gt; None:\n    \"\"\"Creates and prepares a single staging table for data loading.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.base.AbstractNativeLoader.log_run_finish","title":"<code>log_run_finish(log_id, status, records_extracted, records_loaded, error_message=None)</code>  <code>abstractmethod</code>","text":"<p>Logs the completion or failure of an ETL run.</p> Source code in <code>src/py_load_medgen/loader/base.py</code> <pre><code>@abstractmethod\ndef log_run_finish(\n    self,\n    log_id: int,\n    status: str,\n    records_extracted: int,\n    records_loaded: int,\n    error_message: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Logs the completion or failure of an ETL run.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.base.AbstractNativeLoader.log_run_start","title":"<code>log_run_start(run_id, package_version, load_mode, source_files)</code>  <code>abstractmethod</code>","text":"<p>Logs the start of an ETL run and returns a unique identifier for the run log.</p> Source code in <code>src/py_load_medgen/loader/base.py</code> <pre><code>@abstractmethod\ndef log_run_start(\n    self, run_id: uuid.UUID, package_version: str, load_mode: str, source_files: dict\n) -&gt; int:\n    \"\"\"Logs the start of an ETL run and returns a unique identifier for the run log.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.factory","title":"<code>py_load_medgen.loader.factory</code>","text":""},{"location":"reference/#py_load_medgen.loader.factory.LoaderFactory","title":"<code>LoaderFactory</code>","text":"<p>A factory for creating database-specific loader instances. This class is responsible for selecting the correct concrete implementation of AbstractNativeLoader based on the provided database connection string (DSN).</p> Source code in <code>src/py_load_medgen/loader/factory.py</code> <pre><code>class LoaderFactory:\n    \"\"\"\n    A factory for creating database-specific loader instances.\n    This class is responsible for selecting the correct concrete implementation\n    of AbstractNativeLoader based on the provided database connection string (DSN).\n    \"\"\"\n\n    @staticmethod\n    def create_loader(db_dsn: str) -&gt; AbstractNativeLoader:\n        \"\"\"\n        Instantiates the appropriate native loader based on the DSN scheme.\n        Args:\n            db_dsn: The database connection string.\n        Returns:\n            A concrete instance of AbstractNativeLoader.\n        Raises:\n            ValueError: If the DSN scheme is unsupported.\n        \"\"\"\n        try:\n            parsed_uri = urlparse(db_dsn)\n            scheme = parsed_uri.scheme\n        except Exception as e:\n            raise ValueError(f\"Could not parse database DSN: {e}\") from e\n\n        if not scheme:\n            raise ValueError(\"Could not parse database DSN: scheme is missing.\")\n\n        if scheme in (\"postgres\", \"postgresql\"):\n            return PostgresNativeLoader(db_dsn=db_dsn)\n        # Placeholder for future implementations\n        # elif scheme == \"redshift\":\n        #     return RedshiftNativeLoader(db_dsn=db_dsn)\n        else:\n            raise ValueError(f\"Unsupported database scheme: '{scheme}'. Supported schemes are: 'postgresql'.\")\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.factory.LoaderFactory.create_loader","title":"<code>create_loader(db_dsn)</code>  <code>staticmethod</code>","text":"<p>Instantiates the appropriate native loader based on the DSN scheme. Args:     db_dsn: The database connection string. Returns:     A concrete instance of AbstractNativeLoader. Raises:     ValueError: If the DSN scheme is unsupported.</p> Source code in <code>src/py_load_medgen/loader/factory.py</code> <pre><code>@staticmethod\ndef create_loader(db_dsn: str) -&gt; AbstractNativeLoader:\n    \"\"\"\n    Instantiates the appropriate native loader based on the DSN scheme.\n    Args:\n        db_dsn: The database connection string.\n    Returns:\n        A concrete instance of AbstractNativeLoader.\n    Raises:\n        ValueError: If the DSN scheme is unsupported.\n    \"\"\"\n    try:\n        parsed_uri = urlparse(db_dsn)\n        scheme = parsed_uri.scheme\n    except Exception as e:\n        raise ValueError(f\"Could not parse database DSN: {e}\") from e\n\n    if not scheme:\n        raise ValueError(\"Could not parse database DSN: scheme is missing.\")\n\n    if scheme in (\"postgres\", \"postgresql\"):\n        return PostgresNativeLoader(db_dsn=db_dsn)\n    # Placeholder for future implementations\n    # elif scheme == \"redshift\":\n    #     return RedshiftNativeLoader(db_dsn=db_dsn)\n    else:\n        raise ValueError(f\"Unsupported database scheme: '{scheme}'. Supported schemes are: 'postgresql'.\")\n</code></pre>"},{"location":"reference/#postgresql-loader","title":"PostgreSQL Loader","text":"<p>This is the default implementation for loading data into PostgreSQL.</p>"},{"location":"reference/#py_load_medgen.loader.postgres","title":"<code>py_load_medgen.loader.postgres</code>","text":""},{"location":"reference/#py_load_medgen.loader.postgres.PostgresNativeLoader","title":"<code>PostgresNativeLoader</code>","text":"<p>               Bases: <code>AbstractNativeLoader</code></p> <p>A native loader for PostgreSQL that uses the COPY protocol for high-performance data ingestion.</p> Source code in <code>src/py_load_medgen/loader/postgres.py</code> <pre><code>class PostgresNativeLoader(AbstractNativeLoader):\n    \"\"\"\n    A native loader for PostgreSQL that uses the COPY protocol for high-performance\n    data ingestion.\n    \"\"\"\n\n    def __init__(\n        self,\n        db_dsn: Optional[str] = None,\n        connection: Optional[psycopg.Connection] = None,\n        autocommit: bool = True,\n    ):\n        \"\"\"\n        Initializes the PostgreSQL loader.\n        Accepts either a DSN string or an existing connection object.\n        Args:\n            db_dsn: The database connection string (DSN).\n            connection: An existing psycopg.Connection object (useful for testing).\n            autocommit: If True, commits transactions automatically. Set to False\n                        when an external transaction manager is used (e.g., in tests).\n        \"\"\"\n        if not db_dsn and not connection:\n            raise ValueError(\"Either db_dsn or connection must be provided.\")\n\n        self.dsn = db_dsn\n        self.conn = connection\n        self._managed_connection = connection is None\n        self.autocommit = autocommit\n\n    def _commit(self) -&gt; None:\n        \"\"\"Commits the transaction if a connection exists and autocommit is enabled.\"\"\"\n        if self.conn and not self.conn.closed and self.autocommit:\n            self.conn.commit()\n\n    def connect(self) -&gt; None:\n        \"\"\"Establishes a connection and ensures metadata tables exist.\"\"\"\n        if not self.conn or self.conn.closed:\n            if self._managed_connection and self.dsn:\n                try:\n                    logging.info(\"Connecting to PostgreSQL database...\")\n                    self.conn = psycopg.connect(self.dsn)\n                    logging.info(\"Connection successful.\")\n                except psycopg.Error as e:\n                    logging.error(f\"Database connection error: {e}\")\n                    raise\n            elif self.conn and self.conn.closed:\n                raise ConnectionError(\"Managed connection was closed and cannot be reopened without a DSN.\")\n            elif not self._managed_connection:\n                raise ConnectionError(\"The provided external connection is closed.\")\n            else:\n                raise ConnectionError(\"Cannot connect without a DSN.\")\n        self._initialize_metadata()\n\n    def close(self) -&gt; None:\n        \"\"\"Closes the database connection if it was created and is managed by this loader.\"\"\"\n        if self.conn and not self.conn.closed and self._managed_connection:\n            self.conn.close()\n            logging.info(\"Managed database connection closed.\")\n        else:\n            logging.debug(\"Pre-existing connection not closed by loader.\")\n\n    def __enter__(self):\n        \"\"\"Context manager entry point, establishes connection.\"\"\"\n        self.connect()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit point, closes connection.\"\"\"\n        self.close()\n\n    def initialize_staging(self, table_name: str, ddl: str) -&gt; None:\n        \"\"\"Creates and prepares a single staging table for data loading.\"\"\"\n        if not self.conn:\n            raise ConnectionError(\"Database connection not established.\")\n        logging.info(f\"Initializing staging table: {table_name}\")\n        with self.conn.cursor() as cur:\n            cur.execute(f\"DROP TABLE IF EXISTS {table_name} CASCADE;\")\n            cur.execute(ddl)\n        self._commit()\n        logging.info(f\"Staging table {table_name} initialized successfully.\")\n\n    def bulk_load(self, table_name: str, data_iterator: Iterator[bytes]) -&gt; None:\n        \"\"\"Executes a native, high-performance bulk load operation using COPY.\"\"\"\n        if not self.conn:\n            raise ConnectionError(\"Database connection not established.\")\n        logging.info(f\"Starting bulk load into '{table_name}'...\")\n        with self.conn.cursor() as cur:\n            with cur.copy(f\"COPY {table_name} FROM STDIN WITH (FORMAT TEXT, NULL '\\\\N')\") as copy:\n                for line in data_iterator:\n                    copy.write(line)\n        self._commit()\n        logging.info(f\"Bulk load into '{table_name}' complete.\")\n\n    def _initialize_metadata(self) -&gt; None:\n        \"\"\"Ensures the ETL audit log table exists.\"\"\"\n        if not self.conn:\n            raise ConnectionError(\"Database connection not established.\")\n        logging.info(\"Initializing metadata table: etl_audit_log\")\n        with self.conn.cursor() as cur:\n            cur.execute(ETL_AUDIT_LOG_DDL)\n        self._commit()\n        logging.info(\"Metadata table initialized.\")\n\n    def log_run_start(\n        self, run_id: uuid.UUID, package_version: str, load_mode: str, source_files: dict\n    ) -&gt; int:\n        \"\"\"Logs the start of an ETL run and returns the log_id.\"\"\"\n        if not self.conn:\n            raise ConnectionError(\"Database connection not established.\")\n        sql = \"INSERT INTO etl_audit_log (run_id, package_version, load_mode, source_files, start_time, status) VALUES (%s, %s, %s, %s, %s, %s) RETURNING log_id;\"\n        start_time = datetime.now(timezone.utc)\n        source_files_json = psycopg.types.json.Jsonb(source_files)\n        with self.conn.cursor() as cur:\n            cur.execute(sql, (run_id, package_version, load_mode, source_files_json, start_time, \"In Progress\"))\n            log_id = cur.fetchone()[0]\n            self._commit()\n        logging.info(f\"ETL run started. Log ID: {log_id}\")\n        return log_id\n\n    def log_run_finish(\n        self,\n        log_id: int,\n        status: str,\n        records_extracted: int,\n        records_loaded: int,\n        error_message: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Logs the completion or failure of an ETL run.\"\"\"\n        if not self.conn:\n            raise ConnectionError(\"Database connection not established.\")\n        sql = \"UPDATE etl_audit_log SET end_time = %s, status = %s, records_extracted = %s, records_loaded = %s, error_message = %s WHERE log_id = %s;\"\n        end_time = datetime.now(timezone.utc)\n        with self.conn.cursor() as cur:\n            cur.execute(sql, (end_time, status, records_extracted, records_loaded, error_message, log_id))\n            self._commit()\n        logging.info(f\"ETL run finished for Log ID: {log_id}. Status: {status}\")\n\n    def execute_cdc(\n        self, staging_table: str, production_table: str, pk_name: str, business_key: str\n    ) -&gt; dict[str, int]:\n        \"\"\"Executes Change Data Capture (CDC) logic using SQL.\"\"\"\n        if not self.conn:\n            raise ConnectionError(\"Database connection not established.\")\n        logging.info(f\"Executing CDC for {production_table} using key '{business_key}'...\")\n        with self.conn.cursor() as cur:\n            cur.execute(\"CREATE TEMP TABLE IF NOT EXISTS cdc_deletes (id BIGINT) ON COMMIT PRESERVE ROWS;\")\n            cur.execute(f\"CREATE TEMP TABLE IF NOT EXISTS cdc_inserts (LIKE {staging_table} INCLUDING DEFAULTS) ON COMMIT PRESERVE ROWS;\")\n            sql_find_deletes = f\"INSERT INTO cdc_deletes (id) SELECT p.{pk_name} FROM {production_table} p LEFT JOIN {staging_table} s ON p.{business_key} = s.{business_key} WHERE s.{business_key} IS NULL AND p.is_active = true;\"\n            cur.execute(sql_find_deletes)\n            delete_count = cur.rowcount\n            sql_find_inserts = f\"INSERT INTO cdc_inserts SELECT s.* FROM {staging_table} s LEFT JOIN {production_table} p ON s.{business_key} = p.{business_key} AND p.is_active = true WHERE p.{pk_name} IS NULL;\"\n            cur.execute(sql_find_inserts)\n            insert_count = cur.rowcount\n        logging.info(f\"CDC complete. Inserts: {insert_count}, Deletes: {delete_count}\")\n        return {\"inserts\": insert_count, \"deletes\": delete_count}\n\n    def _get_table_indexes(self, table_name: str) -&gt; list[str]:\n        \"\"\"Retrieves the DDL for all non-primary-key indexes on a given table.\"\"\"\n        if not self.conn:\n            raise ConnectionError(\"Database connection not established.\")\n        logging.info(f\"Discovering indexes for table: {table_name}\")\n        sql = \"SELECT indexdef FROM pg_indexes i JOIN pg_class c ON i.indexname = c.relname LEFT JOIN pg_constraint con ON c.oid = con.conindid WHERE i.tablename = %s AND con.contype IS DISTINCT FROM 'p';\"\n        with self.conn.cursor() as cur:\n            try:\n                cur.execute(sql, (table_name,))\n                index_ddls = [row[0] for row in cur.fetchall()]\n                logging.info(f\"Found {len(index_ddls)} non-PK indexes for {table_name}.\")\n                return index_ddls\n            except psycopg.errors.UndefinedTable:\n                logging.warning(f\"Table '{table_name}' does not exist, cannot discover indexes. Returning empty list.\")\n                return []\n\n    def apply_changes(\n        self,\n        mode: str,\n        staging_table: str,\n        production_table: str,\n        production_ddl: str,\n        index_ddls: list[str],\n        pk_name: str,\n        business_key: str,\n    ) -&gt; None:\n        \"\"\"Applies changes to the production table based on the load mode.\"\"\"\n        if mode == \"full\":\n            self._apply_full_load(staging_table, production_table, production_ddl)\n        elif mode == \"delta\":\n            self._apply_delta_load(production_table, pk_name, production_ddl, index_ddls)\n        else:\n            raise ValueError(f\"Unknown load mode: {mode}\")\n\n    def _apply_full_load(\n        self, staging_table: str, production_table: str, production_ddl: str\n    ) -&gt; None:\n        \"\"\"Applies changes atomically using the 'atomic swap' method for a full load.\"\"\"\n        if not self.conn:\n            raise ConnectionError(\"Database connection not established.\")\n        new_production_table = f\"{production_table}_new\"\n        backup_table = f\"{production_table}_old\"\n        index_ddls = self._get_table_indexes(production_table)\n        logging.info(f\"Applying FULL load for table {production_table} with atomic swap...\")\n        with self.conn.cursor() as cur:\n            cur.execute(production_ddl.format(table_name=new_production_table))\n            cur.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name = %s ORDER BY ordinal_position;\", (staging_table,))\n            columns = [row[0] for row in cur.fetchall()]\n            column_list_str = \", \".join(columns)\n            logging.info(f\"Loading data from '{staging_table}' into '{new_production_table}'\")\n            cur.execute(f\"INSERT INTO {new_production_table} ({column_list_str}) SELECT {column_list_str} FROM {staging_table};\")\n            logging.info(f\"Replicating {len(index_ddls)} indexes on new table {new_production_table}...\")\n            for index_ddl in index_ddls:\n                prefix, sep, suffix = index_ddl.rpartition(f\" ON {production_table}\")\n                replicated_ddl = (prefix + f\" ON {new_production_table}\" + suffix) if sep else index_ddl.replace(production_table, new_production_table)\n                cur.execute(replicated_ddl)\n            logging.info(\"Performing atomic swap in a single transaction...\")\n            with self.conn.transaction():\n                cur.execute(f\"DROP TABLE IF EXISTS {backup_table} CASCADE;\")\n                cur.execute(f\"ALTER TABLE IF EXISTS {production_table} RENAME TO {backup_table};\")\n                cur.execute(f\"ALTER TABLE {new_production_table} RENAME TO {production_table};\")\n        logging.info(f\"Atomic swap complete for {production_table}. Production data is updated.\")\n\n    def _apply_delta_load(\n        self, production_table: str, pk_name: str, production_ddl: str, index_ddls: list[str]\n    ) -&gt; None:\n        \"\"\"Applies inserts and soft deletes for a delta load, creating the table if it doesn't exist.\"\"\"\n        if not self.conn:\n            raise ConnectionError(\"Database connection not established.\")\n        logging.info(f\"Applying DELTA load for table {production_table}...\")\n        with self.conn.cursor() as cur:\n            # Ensure production table and indexes exist\n            cur.execute(\"SELECT to_regclass(%s)\", (production_table,))\n            if cur.fetchone()[0] is None:\n                logging.info(f\"Production table '{production_table}' does not exist. Creating now...\")\n                cur.execute(production_ddl.format(table_name=production_table))\n                for index_ddl in index_ddls:\n                    cur.execute(index_ddl.format(table_name=production_table))\n                logging.info(f\"Table '{production_table}' and its indexes created.\")\n\n            cur.execute(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'cdc_inserts' ORDER BY ordinal_position;\")\n            columns = [row[0] for row in cur.fetchall()]\n            column_list_str = \", \".join(columns)\n            with self.conn.transaction():\n                sql_delete = f\"UPDATE {production_table} SET is_active = false, last_updated_at = NOW() WHERE {pk_name} IN (SELECT id FROM cdc_deletes);\"\n                cur.execute(sql_delete)\n                logging.info(f\"Applied {cur.rowcount} soft deletes.\")\n                sql_insert = f\"INSERT INTO {production_table} ({column_list_str}) SELECT {column_list_str} FROM cdc_inserts;\"\n                cur.execute(sql_insert)\n                logging.info(f\"Applied {cur.rowcount} inserts.\")\n        logging.info(f\"Delta load for {production_table} complete.\")\n\n    def cleanup(self, staging_table: str, production_table: str) -&gt; None:\n        \"\"\"Performs cleanup operations by dropping old backup and staging tables.\"\"\"\n        if not self.conn:\n            raise ConnectionError(\"Database connection not established.\")\n        backup_table = f\"{production_table}_old\"\n        logging.info(f\"Performing cleanup for {production_table}...\")\n        with self.conn.cursor() as cur:\n            with self.conn.transaction():\n                cur.execute(f\"DROP TABLE IF EXISTS {backup_table} CASCADE;\")\n                cur.execute(f\"DROP TABLE IF EXISTS {staging_table} CASCADE;\")\n                cur.execute(\"DROP TABLE IF EXISTS cdc_deletes; DROP TABLE IF EXISTS cdc_inserts;\")\n        logging.info(f\"Cleanup complete. Dropped tables: {backup_table}, {staging_table}, cdc_deletes, cdc_inserts\")\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.postgres.PostgresNativeLoader.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry point, establishes connection.</p> Source code in <code>src/py_load_medgen/loader/postgres.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry point, establishes connection.\"\"\"\n    self.connect()\n    return self\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.postgres.PostgresNativeLoader.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit point, closes connection.</p> Source code in <code>src/py_load_medgen/loader/postgres.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit point, closes connection.\"\"\"\n    self.close()\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.postgres.PostgresNativeLoader.__init__","title":"<code>__init__(db_dsn=None, connection=None, autocommit=True)</code>","text":"<p>Initializes the PostgreSQL loader. Accepts either a DSN string or an existing connection object. Args:     db_dsn: The database connection string (DSN).     connection: An existing psycopg.Connection object (useful for testing).     autocommit: If True, commits transactions automatically. Set to False                 when an external transaction manager is used (e.g., in tests).</p> Source code in <code>src/py_load_medgen/loader/postgres.py</code> <pre><code>def __init__(\n    self,\n    db_dsn: Optional[str] = None,\n    connection: Optional[psycopg.Connection] = None,\n    autocommit: bool = True,\n):\n    \"\"\"\n    Initializes the PostgreSQL loader.\n    Accepts either a DSN string or an existing connection object.\n    Args:\n        db_dsn: The database connection string (DSN).\n        connection: An existing psycopg.Connection object (useful for testing).\n        autocommit: If True, commits transactions automatically. Set to False\n                    when an external transaction manager is used (e.g., in tests).\n    \"\"\"\n    if not db_dsn and not connection:\n        raise ValueError(\"Either db_dsn or connection must be provided.\")\n\n    self.dsn = db_dsn\n    self.conn = connection\n    self._managed_connection = connection is None\n    self.autocommit = autocommit\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.postgres.PostgresNativeLoader.apply_changes","title":"<code>apply_changes(mode, staging_table, production_table, production_ddl, index_ddls, pk_name, business_key)</code>","text":"<p>Applies changes to the production table based on the load mode.</p> Source code in <code>src/py_load_medgen/loader/postgres.py</code> <pre><code>def apply_changes(\n    self,\n    mode: str,\n    staging_table: str,\n    production_table: str,\n    production_ddl: str,\n    index_ddls: list[str],\n    pk_name: str,\n    business_key: str,\n) -&gt; None:\n    \"\"\"Applies changes to the production table based on the load mode.\"\"\"\n    if mode == \"full\":\n        self._apply_full_load(staging_table, production_table, production_ddl)\n    elif mode == \"delta\":\n        self._apply_delta_load(production_table, pk_name, production_ddl, index_ddls)\n    else:\n        raise ValueError(f\"Unknown load mode: {mode}\")\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.postgres.PostgresNativeLoader.bulk_load","title":"<code>bulk_load(table_name, data_iterator)</code>","text":"<p>Executes a native, high-performance bulk load operation using COPY.</p> Source code in <code>src/py_load_medgen/loader/postgres.py</code> <pre><code>def bulk_load(self, table_name: str, data_iterator: Iterator[bytes]) -&gt; None:\n    \"\"\"Executes a native, high-performance bulk load operation using COPY.\"\"\"\n    if not self.conn:\n        raise ConnectionError(\"Database connection not established.\")\n    logging.info(f\"Starting bulk load into '{table_name}'...\")\n    with self.conn.cursor() as cur:\n        with cur.copy(f\"COPY {table_name} FROM STDIN WITH (FORMAT TEXT, NULL '\\\\N')\") as copy:\n            for line in data_iterator:\n                copy.write(line)\n    self._commit()\n    logging.info(f\"Bulk load into '{table_name}' complete.\")\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.postgres.PostgresNativeLoader.cleanup","title":"<code>cleanup(staging_table, production_table)</code>","text":"<p>Performs cleanup operations by dropping old backup and staging tables.</p> Source code in <code>src/py_load_medgen/loader/postgres.py</code> <pre><code>def cleanup(self, staging_table: str, production_table: str) -&gt; None:\n    \"\"\"Performs cleanup operations by dropping old backup and staging tables.\"\"\"\n    if not self.conn:\n        raise ConnectionError(\"Database connection not established.\")\n    backup_table = f\"{production_table}_old\"\n    logging.info(f\"Performing cleanup for {production_table}...\")\n    with self.conn.cursor() as cur:\n        with self.conn.transaction():\n            cur.execute(f\"DROP TABLE IF EXISTS {backup_table} CASCADE;\")\n            cur.execute(f\"DROP TABLE IF EXISTS {staging_table} CASCADE;\")\n            cur.execute(\"DROP TABLE IF EXISTS cdc_deletes; DROP TABLE IF EXISTS cdc_inserts;\")\n    logging.info(f\"Cleanup complete. Dropped tables: {backup_table}, {staging_table}, cdc_deletes, cdc_inserts\")\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.postgres.PostgresNativeLoader.close","title":"<code>close()</code>","text":"<p>Closes the database connection if it was created and is managed by this loader.</p> Source code in <code>src/py_load_medgen/loader/postgres.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes the database connection if it was created and is managed by this loader.\"\"\"\n    if self.conn and not self.conn.closed and self._managed_connection:\n        self.conn.close()\n        logging.info(\"Managed database connection closed.\")\n    else:\n        logging.debug(\"Pre-existing connection not closed by loader.\")\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.postgres.PostgresNativeLoader.connect","title":"<code>connect()</code>","text":"<p>Establishes a connection and ensures metadata tables exist.</p> Source code in <code>src/py_load_medgen/loader/postgres.py</code> <pre><code>def connect(self) -&gt; None:\n    \"\"\"Establishes a connection and ensures metadata tables exist.\"\"\"\n    if not self.conn or self.conn.closed:\n        if self._managed_connection and self.dsn:\n            try:\n                logging.info(\"Connecting to PostgreSQL database...\")\n                self.conn = psycopg.connect(self.dsn)\n                logging.info(\"Connection successful.\")\n            except psycopg.Error as e:\n                logging.error(f\"Database connection error: {e}\")\n                raise\n        elif self.conn and self.conn.closed:\n            raise ConnectionError(\"Managed connection was closed and cannot be reopened without a DSN.\")\n        elif not self._managed_connection:\n            raise ConnectionError(\"The provided external connection is closed.\")\n        else:\n            raise ConnectionError(\"Cannot connect without a DSN.\")\n    self._initialize_metadata()\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.postgres.PostgresNativeLoader.execute_cdc","title":"<code>execute_cdc(staging_table, production_table, pk_name, business_key)</code>","text":"<p>Executes Change Data Capture (CDC) logic using SQL.</p> Source code in <code>src/py_load_medgen/loader/postgres.py</code> <pre><code>def execute_cdc(\n    self, staging_table: str, production_table: str, pk_name: str, business_key: str\n) -&gt; dict[str, int]:\n    \"\"\"Executes Change Data Capture (CDC) logic using SQL.\"\"\"\n    if not self.conn:\n        raise ConnectionError(\"Database connection not established.\")\n    logging.info(f\"Executing CDC for {production_table} using key '{business_key}'...\")\n    with self.conn.cursor() as cur:\n        cur.execute(\"CREATE TEMP TABLE IF NOT EXISTS cdc_deletes (id BIGINT) ON COMMIT PRESERVE ROWS;\")\n        cur.execute(f\"CREATE TEMP TABLE IF NOT EXISTS cdc_inserts (LIKE {staging_table} INCLUDING DEFAULTS) ON COMMIT PRESERVE ROWS;\")\n        sql_find_deletes = f\"INSERT INTO cdc_deletes (id) SELECT p.{pk_name} FROM {production_table} p LEFT JOIN {staging_table} s ON p.{business_key} = s.{business_key} WHERE s.{business_key} IS NULL AND p.is_active = true;\"\n        cur.execute(sql_find_deletes)\n        delete_count = cur.rowcount\n        sql_find_inserts = f\"INSERT INTO cdc_inserts SELECT s.* FROM {staging_table} s LEFT JOIN {production_table} p ON s.{business_key} = p.{business_key} AND p.is_active = true WHERE p.{pk_name} IS NULL;\"\n        cur.execute(sql_find_inserts)\n        insert_count = cur.rowcount\n    logging.info(f\"CDC complete. Inserts: {insert_count}, Deletes: {delete_count}\")\n    return {\"inserts\": insert_count, \"deletes\": delete_count}\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.postgres.PostgresNativeLoader.initialize_staging","title":"<code>initialize_staging(table_name, ddl)</code>","text":"<p>Creates and prepares a single staging table for data loading.</p> Source code in <code>src/py_load_medgen/loader/postgres.py</code> <pre><code>def initialize_staging(self, table_name: str, ddl: str) -&gt; None:\n    \"\"\"Creates and prepares a single staging table for data loading.\"\"\"\n    if not self.conn:\n        raise ConnectionError(\"Database connection not established.\")\n    logging.info(f\"Initializing staging table: {table_name}\")\n    with self.conn.cursor() as cur:\n        cur.execute(f\"DROP TABLE IF EXISTS {table_name} CASCADE;\")\n        cur.execute(ddl)\n    self._commit()\n    logging.info(f\"Staging table {table_name} initialized successfully.\")\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.postgres.PostgresNativeLoader.log_run_finish","title":"<code>log_run_finish(log_id, status, records_extracted, records_loaded, error_message=None)</code>","text":"<p>Logs the completion or failure of an ETL run.</p> Source code in <code>src/py_load_medgen/loader/postgres.py</code> <pre><code>def log_run_finish(\n    self,\n    log_id: int,\n    status: str,\n    records_extracted: int,\n    records_loaded: int,\n    error_message: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Logs the completion or failure of an ETL run.\"\"\"\n    if not self.conn:\n        raise ConnectionError(\"Database connection not established.\")\n    sql = \"UPDATE etl_audit_log SET end_time = %s, status = %s, records_extracted = %s, records_loaded = %s, error_message = %s WHERE log_id = %s;\"\n    end_time = datetime.now(timezone.utc)\n    with self.conn.cursor() as cur:\n        cur.execute(sql, (end_time, status, records_extracted, records_loaded, error_message, log_id))\n        self._commit()\n    logging.info(f\"ETL run finished for Log ID: {log_id}. Status: {status}\")\n</code></pre>"},{"location":"reference/#py_load_medgen.loader.postgres.PostgresNativeLoader.log_run_start","title":"<code>log_run_start(run_id, package_version, load_mode, source_files)</code>","text":"<p>Logs the start of an ETL run and returns the log_id.</p> Source code in <code>src/py_load_medgen/loader/postgres.py</code> <pre><code>def log_run_start(\n    self, run_id: uuid.UUID, package_version: str, load_mode: str, source_files: dict\n) -&gt; int:\n    \"\"\"Logs the start of an ETL run and returns the log_id.\"\"\"\n    if not self.conn:\n        raise ConnectionError(\"Database connection not established.\")\n    sql = \"INSERT INTO etl_audit_log (run_id, package_version, load_mode, source_files, start_time, status) VALUES (%s, %s, %s, %s, %s, %s) RETURNING log_id;\"\n    start_time = datetime.now(timezone.utc)\n    source_files_json = psycopg.types.json.Jsonb(source_files)\n    with self.conn.cursor() as cur:\n        cur.execute(sql, (run_id, package_version, load_mode, source_files_json, start_time, \"In Progress\"))\n        log_id = cur.fetchone()[0]\n        self._commit()\n    logging.info(f\"ETL run started. Log ID: {log_id}\")\n    return log_id\n</code></pre>"}]}